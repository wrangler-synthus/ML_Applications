{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "captchas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrangler-synthus/Machine-Learning_Projects/blob/main/Computer%20Vision/OCR%20Model%20for%20Reading%20Captchas/Captchas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avOLETznTBnl"
      },
      "source": [
        "### OCR Model for Reading Captchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpBlYDa2TBnn"
      },
      "source": [
        "#### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujq89x9WTBno"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1E2gXEZUcQm",
        "outputId": "0569f1e4-c51e-445e-e52a-ca5dc26d6935"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh-bEuRITBno"
      },
      "source": [
        "#### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCFyStdpTBnp",
        "outputId": "c343e546-859d-407c-a047-dbaf0dd2f157"
      },
      "source": [
        "# Path to the data directory\n",
        "data_dir = Path(\"/content/gdrive/MyDrive/electoral-tagged\")\n",
        "\n",
        "# Get list of all the images\n",
        "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
        "labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n",
        "characters = set(char for label in labels for char in label)\n",
        "\n",
        "print(\"Number of images found: \", len(images))\n",
        "print(\"Number of labels found: \", len(labels))\n",
        "print(\"Number of unique characters: \", len(characters))\n",
        "print(\"Characters present: \", characters)\n",
        "\n",
        "# Batch size for training and validation\n",
        "batch_size = 1\n",
        "\n",
        "# Desired image dimensions\n",
        "img_width = 42\n",
        "img_height = 132\n",
        "\n",
        "# Factor by which the image is going to be downsampled\n",
        "# by the convolutional blocks. We will be using two\n",
        "# convolution blocks and each block will have\n",
        "# a pooling layer which downsample the features by a factor of 2.\n",
        "# Hence total downsampling factor would be 4.\n",
        "downsample_factor = 4\n",
        "\n",
        "# Maximum length of any captcha in the dataset\n",
        "max_length = max([len(label) for label in labels])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images found:  938\n",
            "Number of labels found:  938\n",
            "Number of unique characters:  9\n",
            "Characters present:  {'3', '6', '2', '1', '4', '8', '5', '7', '9'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-rC85MZTBnq"
      },
      "source": [
        "#### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzAHFkF0TBnq"
      },
      "source": [
        "# Mapping characters to integers\n",
        "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
        ")\n",
        "\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")\n",
        "\n",
        "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
        "    # 1. Get the total size of the dataset\n",
        "    size = len(images)\n",
        "    # 2. Make an indices array and shuffle it, if required\n",
        "    indices = np.arange(size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    # 3. Get the size of training samples\n",
        "    train_samples = int(size * train_size)\n",
        "    # 4. Split data into training and validation sets\n",
        "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
        "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
        "\n",
        "\n",
        "def encode_single_sample(img_path, label):\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [img_height, img_width])\n",
        "    # 5. Transpose the image because we want the time\n",
        "    # dimension to correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    # 6. Map the characters in label to numbers\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
        "    # 7. Return a dict as our model is expecting two inputs\n",
        "    return {\"image\": img, \"label\": label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPolr9JOTBnr"
      },
      "source": [
        "#### Creating Dataset Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSUOIdslTBnr"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = (\n",
        "    train_dataset.map(\n",
        "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(\n",
        "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H25BOkE7TBnr"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM4Jeqf-TBns",
        "outputId": "e4148f52-1a99-4385-ed79-028b6f318ff4"
      },
      "source": [
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # At test time, just return the computed predictions\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    # Inputs to the model\n",
        "    input_img = layers.Input(\n",
        "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
        "    )\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "\n",
        "    # First conv block\n",
        "    x = layers.Conv2D(\n",
        "        32,\n",
        "        (3, 3),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv1\",\n",
        "    )(input_img)Volunteer NSS\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
        "\n",
        "    # Second conv block\n",
        "    x = layers.Conv2D(\n",
        "        64,\n",
        "        (3, 3),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv2\",\n",
        "    )(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
        "\n",
        "    # We have used two max pool with pool size and strides 2.\n",
        "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
        "    # filters in the last layer is 64. Reshape accordingly before\n",
        "    # passing the output to the RNN part of the model\n",
        "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
        "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # RNNs\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
        "\n",
        "    # Add CTC layer for calculating CTC loss at each step\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.models.Model(\n",
        "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
        "    )\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam()\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt)\n",
        "    return model\n",
        "# Get the model\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ocr_model_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 42, 132, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 42, 132, 32)  320         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 21, 66, 32)   0           Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv2 (Conv2D)                  (None, 21, 66, 64)   18496       pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool2 (MaxPooling2D)            (None, 10, 33, 64)   0           Conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 10, 2112)     0           pool2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 10, 64)       135232      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 10, 64)       0           dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 10, 256)      197632      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 10, 128)      164352      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "label (InputLayer)              [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 10, 10)       1290        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "ctc_loss (CTCLayer)             (None, 10, 10)       0           label[0][0]                      \n",
            "                                                                 dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 517,322\n",
            "Trainable params: 517,322\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhHp-fNwTBns"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ5EtGuMTBns",
        "outputId": "8b290bc8-c7a4-4c54-c8e3-6a1e3d6696c2"
      },
      "source": [
        "epochs = 30\n",
        "early_stopping_patience = 3\n",
        "# Add early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "844/844 [==============================] - 19s 14ms/step - loss: 12.6714 - val_loss: 12.4756\n",
            "Epoch 2/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 12.5089 - val_loss: 12.4985\n",
            "Epoch 3/30\n",
            "844/844 [==============================] - 12s 15ms/step - loss: 12.4944 - val_loss: 12.4659\n",
            "Epoch 4/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 12.4810 - val_loss: 12.4925\n",
            "Epoch 5/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 12.4680 - val_loss: 12.4598\n",
            "Epoch 6/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 12.4617 - val_loss: 12.5473\n",
            "Epoch 7/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 12.3924 - val_loss: 11.1473\n",
            "Epoch 8/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 11.6420 - val_loss: 11.0502\n",
            "Epoch 9/30\n",
            "844/844 [==============================] - 11s 14ms/step - loss: 10.9305 - val_loss: 10.1887\n",
            "Epoch 10/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 9.8318 - val_loss: 9.2413\n",
            "Epoch 11/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 9.0986 - val_loss: 8.2456\n",
            "Epoch 12/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 8.4807 - val_loss: 7.4989\n",
            "Epoch 13/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 7.5840 - val_loss: 6.7381\n",
            "Epoch 14/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 6.9244 - val_loss: 6.0077\n",
            "Epoch 15/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 6.4145 - val_loss: 6.0891\n",
            "Epoch 16/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 5.6266 - val_loss: 5.2837\n",
            "Epoch 17/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 5.1351 - val_loss: 4.6439\n",
            "Epoch 18/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 4.7338 - val_loss: 4.5381\n",
            "Epoch 19/30\n",
            "844/844 [==============================] - 11s 14ms/step - loss: 4.2865 - val_loss: 3.9596\n",
            "Epoch 20/30\n",
            "844/844 [==============================] - 11s 14ms/step - loss: 4.0824 - val_loss: 3.9974\n",
            "Epoch 21/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 3.9378 - val_loss: 4.0146\n",
            "Epoch 22/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 3.7122 - val_loss: 3.6207\n",
            "Epoch 23/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 3.3023 - val_loss: 3.5022\n",
            "Epoch 24/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 3.1520 - val_loss: 2.9746\n",
            "Epoch 25/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 2.7620 - val_loss: 2.9009\n",
            "Epoch 26/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 2.5498 - val_loss: 2.8196\n",
            "Epoch 27/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 2.3636 - val_loss: 2.4064\n",
            "Epoch 28/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 2.2541 - val_loss: 2.2077\n",
            "Epoch 29/30\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 1.9095 - val_loss: 2.0978\n",
            "Epoch 30/30\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 1.7829 - val_loss: 1.8121\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}